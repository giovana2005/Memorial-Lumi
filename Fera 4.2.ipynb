{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<span style=\"font-family: 'Palatino Linotype', serif;\">üõë‚ú®üé§Stop right now, thank you very much</span>**\n",
    "----\n",
    "*<span style=\"font-family: 'Angilla Tattoo'\"> \"\"Quando o amor travou o sistema, Maga Patolina, Maga Butterfly e as Spicey Girls enfrentaram o Firewall Fantasma ‚Äî armadas com feiti√ßos de estilo e coragem sincera, para encontrar o C√≥dice do Groove Supremo e reativar o cora√ß√£o do Mainframe.\"\" ü¶Üü¶ãüå∂Ô∏è</span>*\n",
    "\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src = \"Magas spice girls.png\" alt = \"Magas pop\" width = 400>\n",
    "</div>\n",
    "\n",
    "----\n",
    " **Objetivo:** Nesse notebook, produzido pelas alunas Elo√≠sa Maria Amador Souza e Giovana Martins Coelho, iremos implementar uma estrat√©gia de parada antecipada em uma rede neural feita em sala de aula utilizando a biblioteca Pytorch. \n",
    "\n",
    " **O que √© a estrat√©gia de parada antecipada?** A estrat√©gia de parada antecipada calcula a precis√£o de classifia√ß√£o ao final de cada √©poca com os dados de valida√ß√£o, quando a precis√£o da classifica√ß√£o come√ßa a piorar, o treinamento da rede neural termina.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import copy\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_dados_entrada, neuronios_c1, neuronios_c2, num_targets):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.camadas = nn.Sequential(\n",
    "            nn.Linear(num_dados_entrada, neuronios_c1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(neuronios_c1, neuronios_c2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(neuronios_c2, num_targets),\n",
    "        )\n",
    "        \n",
    "    def forward(self, b):\n",
    "        b = self.camadas(b)\n",
    "        return b\n",
    "    \n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAMANHO_TESTE = 0.1\n",
    "TAMANHO_VALIDACAO = 0.1\n",
    "SEMENTE_ALEATORIA = 343"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
      "0    Adelie  Torgersen            39.1           18.7              181.0   \n",
      "1    Adelie  Torgersen            39.5           17.4              186.0   \n",
      "2    Adelie  Torgersen            40.3           18.0              195.0   \n",
      "3    Adelie  Torgersen             NaN            NaN                NaN   \n",
      "4    Adelie  Torgersen            36.7           19.3              193.0   \n",
      "..      ...        ...             ...            ...                ...   \n",
      "339  Gentoo     Biscoe             NaN            NaN                NaN   \n",
      "340  Gentoo     Biscoe            46.8           14.3              215.0   \n",
      "341  Gentoo     Biscoe            50.4           15.7              222.0   \n",
      "342  Gentoo     Biscoe            45.2           14.8              212.0   \n",
      "343  Gentoo     Biscoe            49.9           16.1              213.0   \n",
      "\n",
      "     body_mass_g     sex  \n",
      "0         3750.0    Male  \n",
      "1         3800.0  Female  \n",
      "2         3250.0  Female  \n",
      "3            NaN     NaN  \n",
      "4         3450.0  Female  \n",
      "..           ...     ...  \n",
      "339          NaN     NaN  \n",
      "340       4850.0  Female  \n",
      "341       5750.0    Male  \n",
      "342       5200.0  Female  \n",
      "343       5400.0    Male  \n",
      "\n",
      "[344 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "df_pinguins = sns.load_dataset(\"penguins\")\n",
    "ATRIBUTOS = [\"bill_length_mm\", \"bill_depth_mm\", \"body_mass_g\"]\n",
    "TARGET = [\"flipper_length_mm\"]\n",
    "\n",
    "print(df_pinguins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pinguins = df_pinguins.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = df_pinguins.index\n",
    "indices_treino_val, indices_teste = train_test_split(\n",
    "    indices, test_size=TAMANHO_TESTE, random_state=SEMENTE_ALEATORIA\n",
    ")\n",
    "\n",
    "df_treino_val = df_pinguins.loc[indices_treino_val]\n",
    "df_teste = df_pinguins.loc[indices_teste]\n",
    "\n",
    "X_teste = df_teste.reindex(ATRIBUTOS, axis=1).values\n",
    "y_teste = df_teste.reindex(TARGET, axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = df_treino_val.index\n",
    "indices_treino, indices_val = train_test_split(\n",
    "    indices, test_size=TAMANHO_TESTE, random_state=SEMENTE_ALEATORIA\n",
    ")\n",
    "\n",
    "df_treino = df_pinguins.loc[indices_treino]\n",
    "df_val = df_pinguins.loc[indices_val]\n",
    "\n",
    "X_treino = df_treino.reindex(ATRIBUTOS, axis=1).values\n",
    "y_treino = df_treino.reindex(TARGET, axis=1).values\n",
    "\n",
    "X_val = df_val.reindex(ATRIBUTOS, axis=1).values\n",
    "y_val = df_val.reindex(TARGET, axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler = MaxAbsScaler()\n",
    "x_scaler.fit(X_treino)\n",
    "\n",
    "y_scaler = MaxAbsScaler()\n",
    "y_scaler.fit(y_treino)\n",
    "\n",
    "X_treino = x_scaler.transform(X_treino)\n",
    "y_treino = y_scaler.transform(y_treino)\n",
    "\n",
    "X_val = x_scaler.transform(X_val)\n",
    "y_val = y_scaler.transform(y_val)\n",
    "\n",
    "X_teste = x_scaler.transform(X_teste)\n",
    "y_teste = y_scaler.transform(y_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_teste = torch.tensor(X_teste).float()\n",
    "X_treino = torch.tensor(X_treino).float()\n",
    "X_val = torch.tensor(X_val).float()\n",
    "Y_teste = torch.tensor(y_teste).float()\n",
    "Y_treino = torch.tensor(y_treino).float()\n",
    "Y_val = torch.tensor(y_val).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DADOS_DE_ENTRADA = len(ATRIBUTOS)\n",
    "NUM_DADOS_DE_SAIDA = len(TARGET) \n",
    "NEURONIOS_C1 = 10\n",
    "NEURONIOS_C2 = 8\n",
    "\n",
    "minha_mlp = MLP(\n",
    "    NUM_DADOS_DE_ENTRADA, NEURONIOS_C1, NEURONIOS_C2, NUM_DADOS_DE_SAIDA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXA_DE_APRENDIZADO = 0.45\n",
    "\n",
    "otimizador = optim.SGD(minha_mlp.parameters(), lr=TAXA_DE_APRENDIZADO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_perda = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_function(engine):\n",
    "    val_loss = engine.state.metrics['val_loss']\n",
    "    return -val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.052927467972040176\n",
      "1 0.03494558483362198\n",
      "2 0.007544109132140875\n",
      "3 0.005751939956098795\n",
      "4 0.0032805735245347023\n",
      "5 0.0032805735245347023\n",
      "6 0.0030458499677479267\n",
      "7 0.0030458499677479267\n",
      "8 0.003030559280887246\n",
      "9 0.003030559280887246\n",
      "10 0.0029961285181343555\n",
      "11 0.0029784839134663343\n",
      "12 0.002940878737717867\n",
      "13 0.002913328120484948\n",
      "14 0.0028797141276299953\n",
      "15 0.0028492645360529423\n",
      "16 0.002817192580550909\n",
      "17 0.002786391880363226\n",
      "18 0.002754362067207694\n",
      "19 0.0027206684462726116\n",
      "20 0.002683988306671381\n",
      "21 0.00264498102478683\n",
      "22 0.0026059008669108152\n",
      "23 0.0025666134897619486\n",
      "24 0.0025271750055253506\n",
      "25 0.002487744437530637\n",
      "26 0.002448413986712694\n",
      "27 0.002409011824056506\n",
      "28 0.002369929337874055\n",
      "29 0.0023310657124966383\n",
      "30 0.0022923091892153025\n",
      "31 0.002253703773021698\n",
      "32 0.0022152739111334085\n",
      "33 0.002177175600081682\n",
      "34 0.002142141107469797\n",
      "35 0.002109046094119549\n",
      "36 0.002077271696180105\n",
      "37 0.0020458886865526438\n",
      "38 0.002014933852478862\n",
      "39 0.0019842716865241528\n",
      "40 0.0019538807682693005\n",
      "41 0.0019239840330556035\n",
      "42 0.0018946850905194879\n",
      "43 0.0018672432051971555\n",
      "44 0.0018419658299535513\n",
      "45 0.0018179576145485044\n",
      "46 0.0017943414859473705\n",
      "47 0.0017713462002575397\n",
      "48 0.0017490297323092818\n",
      "49 0.0017276619328185916\n",
      "50 0.001707203802652657\n",
      "51 0.0016887260135263205\n",
      "52 0.001672039506956935\n",
      "53 0.0016556220361962914\n",
      "54 0.0016396213322877884\n",
      "55 0.0016236326191574335\n",
      "56 0.001608099672012031\n",
      "57 0.00159253622405231\n",
      "58 0.001577262650243938\n",
      "59 0.0015624500811100006\n",
      "60 0.0015477756969630718\n",
      "61 0.0015338138910010457\n",
      "62 0.0015201260102912784\n",
      "63 0.0015068418579176068\n",
      "64 0.0014934524660930037\n",
      "65 0.0014806282706558704\n",
      "66 0.0014680175809189677\n",
      "67 0.001455494319088757\n",
      "68 0.001443227520212531\n",
      "69 0.00143101392313838\n",
      "70 0.001419094973243773\n",
      "71 0.001407200237736106\n",
      "72 0.0013954628957435489\n",
      "73 0.001383800758048892\n",
      "74 0.0013722903095185757\n",
      "75 0.001360841328278184\n",
      "76 0.001349517609924078\n",
      "77 0.0013383228797465563\n",
      "78 0.001327346544712782\n",
      "79 0.0013163607800379395\n",
      "80 0.001305564772337675\n",
      "81 0.0012948563089594245\n",
      "82 0.001284401980228722\n",
      "83 0.0012739392695948482\n",
      "84 0.0012636393075808883\n",
      "85 0.0012533803237602115\n",
      "86 0.0012432574294507504\n",
      "87 0.0012332233600318432\n",
      "88 0.0012234329478815198\n",
      "89 0.0012139168102294207\n",
      "90 0.0012045778566971421\n",
      "91 0.0011952962959185243\n",
      "92 0.0011859812075272202\n",
      "93 0.0011768423719331622\n",
      "94 0.001167754177004099\n",
      "95 0.0011588075431063771\n",
      "96 0.0011499273823574185\n",
      "97 0.001141183078289032\n",
      "98 0.0011325511150062084\n",
      "99 0.0011240202002227306\n",
      "100 0.0011155959218740463\n",
      "101 0.0011072525521740317\n",
      "102 0.0010990346781909466\n",
      "103 0.0010909525444731116\n",
      "104 0.0010830133687704802\n",
      "105 0.0010748691856861115\n",
      "106 0.0010667607421055436\n",
      "107 0.001058749738149345\n",
      "108 0.0010508325649425387\n",
      "109 0.0010431044502183795\n",
      "110 0.001035563531331718\n",
      "111 0.0010280528804287314\n",
      "112 0.0010205910075455904\n",
      "113 0.001013139495626092\n",
      "114 0.0010057872859761119\n",
      "115 0.0009985152864828706\n",
      "116 0.0009912429377436638\n",
      "117 0.000984105048701167\n",
      "118 0.000977099989540875\n",
      "119 0.0009702183306217194\n",
      "120 0.0009634252055548131\n",
      "121 0.0009567543747834861\n",
      "122 0.0009502113098278642\n",
      "123 0.0009437265689484775\n",
      "124 0.000937186530791223\n",
      "125 0.0009307086584158242\n",
      "126 0.000924285443034023\n",
      "127 0.0009179693879559636\n",
      "128 0.0009117068257182837\n",
      "129 0.0009055471746250987\n",
      "130 0.0008994116797111928\n",
      "131 0.0008930985350161791\n",
      "132 0.0008869200828485191\n",
      "133 0.0008808603743091226\n",
      "134 0.000874876684974879\n",
      "135 0.0008690571994520724\n",
      "136 0.0008632998215034604\n",
      "137 0.0008575952961109579\n",
      "138 0.0008521684794686735\n",
      "139 0.0008467192528769374\n",
      "140 0.0008413646719418466\n",
      "141 0.0008361479849554598\n",
      "142 0.0008310384000651538\n",
      "143 0.0008259258465841413\n",
      "144 0.0008210671367123723\n",
      "145 0.0008163236198015511\n",
      "146 0.0008116252138279378\n",
      "147 0.000807186821475625\n",
      "148 0.0008029292803257704\n",
      "149 0.0007987701683305204\n",
      "150 0.0007946703117340803\n",
      "151 0.0007906781393103302\n",
      "152 0.0007867738022468984\n",
      "153 0.0007829358801245689\n",
      "154 0.0007791591342538595\n",
      "155 0.0007754700491204858\n",
      "156 0.00077183882240206\n",
      "157 0.000768352416343987\n",
      "158 0.0007648784085176885\n",
      "159 0.0007615183712914586\n",
      "160 0.0007582288817502558\n",
      "161 0.000755042361561209\n",
      "162 0.0007518827333115041\n",
      "163 0.0007488138508051634\n",
      "164 0.0007459015469066799\n",
      "165 0.0007432969287037849\n",
      "166 0.0007407043594866991\n",
      "167 0.0007381513714790344\n",
      "168 0.0007355542620643973\n",
      "169 0.0007330726948566735\n",
      "170 0.0007306887418963015\n",
      "171 0.0007284045568667352\n",
      "172 0.0007261260179802775\n",
      "173 0.0007239556289277971\n",
      "174 0.0007218656246550381\n",
      "175 0.0007198478560894728\n",
      "176 0.0007179121021181345\n",
      "177 0.000716033682692796\n",
      "178 0.0007141849491745234\n",
      "179 0.0007124278927221894\n",
      "180 0.0007106905686669052\n",
      "181 0.0007090079016052186\n",
      "182 0.0007073722663335502\n",
      "183 0.0007057799375616014\n",
      "184 0.0007042146753519773\n",
      "185 0.0007026896346360445\n",
      "186 0.0007012138376012444\n",
      "187 0.0006997697055339813\n",
      "188 0.0006983500788919628\n",
      "189 0.0006969587411731482\n",
      "190 0.0006955990684218705\n",
      "191 0.0006942662876099348\n",
      "192 0.0006929837400093675\n",
      "193 0.0006917386781424284\n",
      "194 0.0006904915207996964\n",
      "195 0.0006892806268297136\n",
      "196 0.0006881526205688715\n",
      "197 0.0006870201323181391\n",
      "198 0.0006859318818897009\n",
      "199 0.0006848478224128485\n",
      "200 0.0006838224944658577\n",
      "201 0.000682793150190264\n",
      "202 0.000681809731759131\n",
      "203 0.000680888828355819\n",
      "204 0.0006800098344683647\n",
      "205 0.0006791226333007216\n",
      "206 0.0006782786804251373\n",
      "207 0.0006774290231987834\n",
      "208 0.0006766248261556029\n",
      "209 0.0006758452509529889\n",
      "210 0.0006750798202119768\n",
      "211 0.0006743324338458478\n",
      "212 0.0006735973292961717\n",
      "213 0.000672878697514534\n",
      "214 0.0006721728714182973\n",
      "215 0.0006714883493259549\n",
      "216 0.0006708118016831577\n",
      "217 0.0006701549864374101\n",
      "218 0.0006695050396956503\n",
      "219 0.0006688793655484915\n",
      "220 0.0006682554958388209\n",
      "221 0.0006676411721855402\n",
      "222 0.0006670423899777234\n",
      "223 0.0006664533284492791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224 0.0006658780621364713\n",
      "225 0.0006653114687651396\n",
      "226 0.000664748135022819\n",
      "227 0.0006642340449616313\n",
      "228 0.0006638052291236818\n",
      "229 0.0006634107558056712\n",
      "230 0.0006630001589655876\n",
      "231 0.000662623904645443\n",
      "232 0.0006622336804866791\n",
      "233 0.0006618749466724694\n",
      "234 0.0006615036982111633\n",
      "235 0.0006611653952859342\n",
      "236 0.0006608086405321956\n",
      "237 0.0006604854133911431\n",
      "238 0.0006601496716029942\n",
      "239 0.0006598280742764473\n",
      "240 0.0006595214363187551\n",
      "241 0.0006592075806111097\n",
      "242 0.0006589136319234967\n",
      "243 0.0006586136878468096\n",
      "244 0.000658333592582494\n",
      "245 0.0006580472690984607\n",
      "246 0.0006576764862984419\n",
      "247 0.0006573821301572025\n",
      "248 0.0006570104160346091\n",
      "249 0.000656727934256196\n",
      "250 0.0006563701899722219\n",
      "251 0.0006560978363268077\n",
      "252 0.000655754585750401\n",
      "253 0.0006554920109920204\n",
      "254 0.00065516063477844\n",
      "255 0.0006549077806994319\n",
      "256 0.0006545901414938271\n",
      "257 0.0006543454364873469\n",
      "258 0.000654039264190942\n",
      "259 0.0006538036977872252\n",
      "260 0.0006535163847729564\n",
      "261 0.0006532917032018304\n",
      "262 0.0006530146929435432\n",
      "263 0.0006527981604449451\n",
      "264 0.0006525309872813523\n",
      "265 0.0006523218471556902\n",
      "266 0.0006520655006170273\n",
      "267 0.0006518640439026058\n",
      "268 0.0006516162538900971\n",
      "269 0.0006514220149256289\n",
      "270 0.0006511828978545964\n",
      "271 0.0006509951781481504\n",
      "272 0.0006507642101496458\n",
      "273 0.0006505464552901685\n",
      "274 0.0006503052427433431\n",
      "275 0.0006501146126538515\n",
      "276 0.0006498690927401185\n",
      "277 0.000649693829473108\n",
      "278 0.0006494679255411029\n",
      "279 0.0006492991815321147\n",
      "280 0.0006490803207270801\n",
      "281 0.0006489183288067579\n",
      "282 0.0006487076752819121\n",
      "283 0.0006485516205430031\n",
      "284 0.000648347893729806\n",
      "285 0.0006481979507952929\n",
      "286 0.0006480018491856754\n",
      "287 0.0006478566210716963\n",
      "288 0.0006476544658653438\n",
      "289 0.0006475254776887596\n",
      "290 0.0006473322864621878\n",
      "291 0.0006472078384831548\n",
      "292 0.000647021341137588\n",
      "293 0.0006469012005254626\n",
      "294 0.0006467232597060502\n",
      "295 0.0006466018385253847\n",
      "296 0.0006464346079155803\n",
      "297 0.0006463050376623869\n",
      "298 0.0006461426964960992\n",
      "299 0.0006460881559178233\n",
      "300 0.0006459447322413325\n",
      "301 0.0006458928692154586\n",
      "302 0.0006457537529058754\n",
      "303 0.0006457048002630472\n",
      "304 0.0006455691182054579\n",
      "305 0.0006455238326452672\n",
      "306 0.0006453794194385409\n",
      "307 0.0006453458336181939\n",
      "308 0.0006452076486311853\n",
      "309 0.0006451767985709012\n",
      "310 0.0006450622458942235\n",
      "311 0.0006450375076383352\n",
      "312 0.0006449265056289732\n",
      "313 0.0006449033971875906\n",
      "314 0.0006447952473536134\n",
      "315 0.0006447733612731099\n",
      "316 0.0006446681218221784\n",
      "317 0.000644648855086416\n",
      "318 0.0006445461185649037\n",
      "319 0.0006445281323976815\n",
      "320 0.0006444274913519621\n",
      "321 0.0006444110767915845\n",
      "322 0.0006443134043365717\n",
      "323 0.0006442989688366652\n",
      "324 0.000644202867988497\n",
      "325 0.0006441898876801133\n",
      "326 0.0006440955912694335\n",
      "327 0.0006440836586989462\n",
      "328 0.0006439915741793811\n",
      "329 0.0006439815042540431\n",
      "330 0.0006437890115194023\n",
      "331 0.0006437570555135608\n",
      "332 0.0006435669492930174\n",
      "333 0.0006435375544242561\n",
      "334 0.0006433512317016721\n",
      "335 0.0006433233502320945\n",
      "336 0.0006431393558159471\n",
      "337 0.0006431143265217543\n",
      "338 0.0006429323111660779\n",
      "339 0.000642908678855747\n",
      "340 0.0006427295738831162\n",
      "341 0.0006427087355405092\n",
      "342 0.0006425312603823841\n",
      "343 0.0006425119354389608\n",
      "344 0.0006423367303796113\n",
      "345 0.0006423199083656073\n",
      "346 0.0006421461584977806\n",
      "347 0.0006421305588446558\n",
      "348 0.0006419612327590585\n",
      "349 0.0006419480196200311\n",
      "350 0.0006417807308025658\n",
      "351 0.0006417682743631303\n",
      "352 0.0006416026153601706\n",
      "353 0.0006415921961888671\n",
      "354 0.0006414286908693612\n",
      "355 0.0006414201343432069\n",
      "356 0.0006412568036466837\n",
      "357 0.0006412508664652705\n",
      "358 0.0006410885835066438\n",
      "359 0.0006410885835066438\n",
      "360 0.0006410212954506278\n",
      "361 0.0006410212954506278\n",
      "362 0.0006409675697796047\n",
      "363 0.0006409675697796047\n",
      "364 0.0006409139605239034\n",
      "365 0.0006409139605239034\n",
      "366 0.0006408611661754549\n",
      "367 0.0006408611661754549\n",
      "368 0.0006408170447684824\n",
      "369 0.0006408170447684824\n",
      "370 0.0006407735636457801\n",
      "371 0.0006407735636457801\n",
      "372 0.0006407263572327793\n",
      "373 0.0006407263572327793\n",
      "374 0.0006406792090274394\n",
      "375 0.0006406792090274394\n",
      "376 0.0006406331085599959\n",
      "377 0.0006406331085599959\n",
      "378 0.0006405874737538397\n",
      "379 0.0006405874737538397\n",
      "380 0.0006405423046089709\n",
      "381 0.0006405423046089709\n",
      "382 0.0006404974265024066\n",
      "383 0.0006404974265024066\n",
      "384 0.0006404524319805205\n",
      "385 0.0006404524319805205\n",
      "386 0.0006404171581380069\n",
      "387 0.0006404171581380069\n",
      "388 0.0006403836305253208\n",
      "389 0.0006403836305253208\n",
      "390 0.0006403502775356174\n",
      "391 0.0006403502775356174\n",
      "392 0.0006403502775356174\n",
      "393 0.0006403502775356174\n",
      "394 0.0006403502775356174\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCAS = 500\n",
    "melhor_perda  = float('inf')\n",
    "melhor_peso = None\n",
    "paciencia = 5\n",
    "\n",
    "for epoca in range(NUM_EPOCAS):\n",
    "    minha_mlp.train()\n",
    "\n",
    "    # forward pass\n",
    "    y_pred = minha_mlp(X_treino)\n",
    "\n",
    "    # zero grad\n",
    "    otimizador.zero_grad()\n",
    "\n",
    "    # loss\n",
    "    loss = fn_perda(Y_treino, y_pred)\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # atualiza par√¢metros\n",
    "    otimizador.step()\n",
    "\n",
    "    minha_mlp.eval()\n",
    "    with torch.no_grad():\n",
    "        valor_output = minha_mlp(X_val)\n",
    "        valor_perda = fn_perda(valor_output, Y_val.float())  \n",
    "        # y_plot.append(valor_perda.item())\n",
    "        # x_plot.append(epoca)\n",
    "\n",
    "    if valor_perda.item() < melhor_perda:\n",
    "        melhor_perda = valor_perda.item()\n",
    "        melhor_peso = copy.deepcopy(minha_mlp.state_dict())\n",
    "        paciencia = 5\n",
    "    else:\n",
    "        paciencia -= 1\n",
    "        if paciencia == 0:\n",
    "            break\n",
    "\n",
    "\n",
    "    print(epoca, melhor_perda)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7977],\n",
       "        [0.9500],\n",
       "        [0.9635],\n",
       "        [0.8379],\n",
       "        [0.9422],\n",
       "        [0.9539],\n",
       "        [0.8206],\n",
       "        [0.8610],\n",
       "        [0.9240],\n",
       "        [0.9431],\n",
       "        [0.9185],\n",
       "        [0.8185],\n",
       "        [0.8505],\n",
       "        [0.8266],\n",
       "        [0.9393],\n",
       "        [0.8528],\n",
       "        [0.8312],\n",
       "        [0.8312],\n",
       "        [0.8177],\n",
       "        [0.9558],\n",
       "        [0.9275],\n",
       "        [0.8543],\n",
       "        [0.7947],\n",
       "        [0.8158],\n",
       "        [0.8081],\n",
       "        [0.8078],\n",
       "        [0.8254],\n",
       "        [0.8585],\n",
       "        [0.8276],\n",
       "        [0.8734],\n",
       "        [0.8605],\n",
       "        [0.8329],\n",
       "        [0.8357],\n",
       "        [0.8048]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prev = minha_mlp(X_teste)\n",
    "y_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### üìä Conclus√£o:\n",
    "\n",
    "Nesse notebook, exploramos a ferramentas de *Early stopping*, que √© uma estrat√©gia √∫til para evitar o *overfitting* das redes neurais. Isso √© importante pois nos ajuda a obter uma previs√£o adequada dos dados, al√©m de diminuir o custo computacional do treinamento da rede, tornando o processo mais eficiente.\n",
    "\n",
    "----\n",
    "### üìö Refer√™ncias:\n",
    "\n",
    "1. SOUZA, Vin√≠cius. Usando Early Stopping para definir o n√∫mero de √©pocas de treinamento. 2021. Dispon√≠vel em: <https://www.deeplearningbook.com.br/usando-early-stopping-para-definir-o-numero-de-epocas-de-treinamento/>. Acesso em: 13 abr. 2025.\n",
    "\n",
    "2. USER12717497. Early stopping in PyTorch. Stack Overflow, 6 abr. 2022. Dispon√≠vel em: <https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch>. Acesso em: 13 abr. 2025.\n",
    "\n",
    "3. BHATT, Vrunda. A step-by-step guide to early stopping in TensorFlow and PyTorch. Medium, 4 jul. 2023. Dispon√≠vel em: <https://medium.com/@vrunda.bhattbhatt/a-step-by-step-guide-to-early-stopping-in-tensorflow-and-pytorch-59c1e3d0e376>. Acesso em: 13 abr. 2025.\n",
    "\n",
    "4. OPENAI. ChatGPT. 2025. Dispon√≠vel em: <https://chatgpt.com/share/67fbf75f-8500-8005-8251-84493623cf41>. Acesso em: 13 abr. 2025."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
